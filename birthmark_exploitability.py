from sklearn.model_selection import train_test_split
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from sklearn.svm import SVC
from sklearn.linear_model import Ridge
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error, r2_score
from xgboost import XGBClassifier
import xgboost as xgb
from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, \
    average_precision_score, classification_report, fbeta_score, make_scorer, precision_recall_curve
from imblearn.over_sampling import SMOTE
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold, GridSearchCV
import nltk
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer
from scipy.sparse import hstack
import re

nltk.download('wordnet')
nltk.download('omw-1.4')

lemmatizer = WordNetLemmatizer()

nltk.download('punkt')
nltk.download('stopwords')

LABEL = 'exploited_in_wild'

def gridsearch(X, y):
    # Gridsearch tries training the model with all parameter options and gives the best one as output
    ftwo_scorer = make_scorer(fbeta_score, beta=1)
    param_grid = {
        'n_estimators': [200, 300, 350],
        'max_features': ['log2', 'sqrt'],
        'max_depth': [30, 40, 50, 60, 70],
        'min_samples_split': [15, 20, 25, 30],
        'min_samples_leaf': [6, 10, 14]
    }
    clf = RandomForestClassifier(class_weight="balanced")
    CV_clf = GridSearchCV(estimator=clf, param_grid=param_grid, cv=10, n_jobs=-1, scoring=ftwo_scorer)
    CV_clf.fit(X, y)

    return CV_clf.best_params_

def to_labels(pos_probs, threshold):
    return (pos_probs >= threshold).astype("int")

def find_threshold(y_test, pos_probas):
    # This function finds the best threshold (instead of 0.5) that gives us a
    # better recall, by using the fbeta (F-score) to evaluate.
    _, _, thresholds = precision_recall_curve(y_test, pos_probas)

    thresholds = np.arange(0, 1, 0.001)
    scores = [fbeta_score(y_test, to_labels(pos_probas, t), beta=1) for t in thresholds]

    ix = np.argmax(scores)
    return thresholds[ix]

def find_threshold_with_all_exploited(y_test, pos_probas):
    # Iterate over a range of thresholds to find the one where all 1s are correctly identified
    best_threshold = 0.5  # Default threshold for binary classification
    max_accuracy = 0
    for threshold in np.linspace(1, 0, 1000):  # Adjust the range and step size as necessary
        # Convert probabilities to binary predictions based on the current threshold
        y_pred = (pos_probas >= threshold).astype(int)

        # Calculate accuracy and other metrics if needed
        current_accuracy = accuracy_score(y_test, y_pred)

        # Check if all 1s are correctly identified
        if np.all((y_pred[y_test == 1] == 1)):
            if current_accuracy > max_accuracy:
                max_accuracy = current_accuracy
                best_threshold = threshold
                num_predictions_above_threshold = sum(y_pred)  # 1857

    # Output the best threshold and its accuracy
    print(f"Best Threshold: {best_threshold}")
    print(f"Accuracy at this threshold: {max_accuracy}")
    print(f"Number of predictions above the threshold: {num_predictions_above_threshold}")
    return best_threshold, num_predictions_above_threshold, max_accuracy

def evaluate(n_splits, X, y):
    # Parameters found in a previous gridsearch
    model_parameters = {'max_depth': 70, 'max_features': 'sqrt', 'min_samples_leaf': 6, 'min_samples_split': 25,
                        'n_estimators': 200}
    use_gridsearch = False

    if use_gridsearch:
        print("Starting gridsearch.. This might take a while")
        model_parameters = gridsearch(X, y)
        print("Gridsearch parameters:")
        print(model_parameters)

    clf = RandomForestClassifier(class_weight="balanced", **model_parameters)

    kfold = StratifiedKFold(n_splits=n_splits, shuffle=True)

    accuracy = []
    precision = []
    recall = []
    f1 = []

    folds_thresholds = []

    print("Starting 10-fold cross validation")

    # We do 10-fold cross-validation to have the most accurate evaluation.
    # Accuracy, precison, recall and f1 score are used and the average is printed
    for train, test in kfold.split(X, y):
        clf.fit(X[train], y[train])
        probas = clf.predict_proba(X[test])
        pos_probas = probas[:, 1]

        # Find the best threshold for this fold
        threshold = find_threshold(y[test], pos_probas)
        folds_thresholds.append(threshold)

        prediction = []
        for proba in probas:
            if proba[1] < threshold:
                prediction.append(0)
            else:
                prediction.append(1)

        scores = clf.score(X[test], y[test])

        accuracy.append(scores * 100)
        precision.append(precision_score(y[test], prediction) * 100)
        recall.append(recall_score(y[test], prediction) * 100)
        f1.append(fbeta_score(y[test], prediction, beta=1) * 100)

    print(f"Accuracy: %.2f%% (+/- %.2f%%)" % (np.mean(accuracy), np.std(accuracy)))
    print("Precision: %.2f%% (+/- %.2f%%)" % (np.mean(precision), np.std(precision)))
    print("Recall: %.2f%% (+/- %.2f%%)" % (np.mean(recall), np.std(recall)))
    print("F1 score: %.2f%% (+/- %.2f%%)" % (np.mean(f1), np.std(f1)))
    print("Threshold: %.2f" % (np.mean(folds_thresholds)))

    # Return the average threshold to be used in the model fit
    return np.mean(folds_thresholds)

def evaluate_svm(n_splits, X, y):
    # Parameters for SVM (these would ideally be found using a grid search)
    model_parameters = {'C': 1.0, 'kernel': 'rbf', 'gamma': 'scale'}
    use_gridsearch = False

    if use_gridsearch:
        print("Starting gridsearch.. This might take a while")
        model_parameters = gridsearch(X, y)  # Assuming gridsearch is a function defined elsewhere
        print("Gridsearch parameters:")
        print(model_parameters)

    clf = SVC(class_weight="balanced", probability=True, **model_parameters)

    kfold = StratifiedKFold(n_splits=n_splits, shuffle=True)

    accuracy = []
    precision = []
    recall = []
    f1 = []

    folds_thresholds = []

    print("Starting cross-validation")

    # We do cross-validation to have the most accurate evaluation.
    # Accuracy, precision, recall, and f1 score are used, and the average is printed
    for train, test in kfold.split(X, y):
        clf.fit(X[train], y[train])
        predictions = clf.predict(X[test])

        accuracy.append(clf.score(X[test], y[test]) * 100)
        precision.append(precision_score(y[test], predictions) * 100)
        recall.append(recall_score(y[test], predictions) * 100)
        f1.append(fbeta_score(y[test], predictions, beta=1) * 100)

    print(f"Accuracy: {np.mean(accuracy):.2f}% (+/- {np.std(accuracy):.2f}%)")
    print(f"Precision: {np.mean(precision):.2f}% (+/- {np.std(precision):.2f}%)")
    print(f"Recall: {np.mean(recall):.2f}% (+/- {np.std(recall):.2f}%)")
    print(f"F1 score: {np.mean(f1):.2f}% (+/- {np.std(f1):.2f}%)")

    # Return any additional information if needed
    return {
        'accuracy': np.mean(accuracy),
        'precision': np.mean(precision),
        'recall': np.mean(recall),
        'f1': np.mean(f1),
    }

def preprocess_text(text):
    # Lowercasing
    text = text.lower()
    # Tokenization
    words = word_tokenize(text)
    # Removing Stop Words and Punctuation
    words = [word for word in words if word.isalpha() and word not in stopwords.words('english')]
    # Stemming
    stemmer = PorterStemmer()
    words = [stemmer.stem(word) for word in words]
    return ' '.join(words)

def tfidf_features(df):
    tfidf_vectorizer = TfidfVectorizer()
    tfidf_matrix = tfidf_vectorizer.fit_transform(df['clean_summary'])
    tfidf_dense = tfidf_matrix.toarray()

    # combined_features = np.concatenate([tfidf_dense, other_features.values], axis=1)
    X = tfidf_matrix  # combined_features# Features
    # save and read matrix into/from a file
    # Convert to DataFrame
    df = pd.DataFrame(tfidf_dense)

    # Save to CSV
    df.to_csv('tfidf_matrix.csv', index=False)
    # Read from CSV
    df = pd.read_csv('tfidf_matrix.csv')

    # Convert DataFrame to numpy array if necessary
    tfidf_dense = df.values


    # Froncious code
    df[LABEL] = df[['clam_report', 'cisa_report', 'secureworks_report', 'graynoise_report']].any(axis=1).astype(
        int)
    y = df[LABEL]  # Target

    # Model training and evaluation
    evaluate(10, X, y)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # model training
    model = RandomForestClassifier()
    model.fit(X_train, y_train)
    # 10 fold cross validation
    # gradian boost
    # model evaluatuon
    y_pred = model.predict(X_test)
    print(accuracy_score(y_test, y_pred))
    print(classification_report(y_test, y_pred))

def tfidf_top_feature(df, top_n):
    # Assuming df is your DataFrame
    # df = pd.DataFrame({'summary': [...], 'label': [...]})

    # Split the DataFrame into two based on the label
    df_class_0 = df[df[LABEL] == 0]
    df_class_1 = df[df[LABEL] == 1]

    # Initialize TF-IDF Vectorizer
    vectorizer = TfidfVectorizer(ngram_range=(1, 2))

    # Fit the vectorizer on the entire dataset to get a consistent set of features
    vectorizer.fit(df['summary_mapped'])

    # Transform the subsets
    tfidf_0 = vectorizer.transform(df_class_0['summary_mapped'])
    tfidf_1 = vectorizer.transform(df_class_1['summary_mapped'])

    # Get feature names
    feature_names = vectorizer.get_feature_names_out()

    # Get average TF-IDF score for each word in each class
    avg_scores_0 = np.mean(tfidf_0, axis=0)
    avg_scores_1 = np.mean(tfidf_1, axis=0)

    # Create a DataFrame for comparison
    df_comparison = pd.DataFrame([avg_scores_0.A1, avg_scores_1.A1],
                                 index=['Class 0', 'Class 1'],
                                 columns=feature_names).transpose()

    # Calculate the difference in scores
    df_comparison['difference'] = df_comparison['Class 1'] - df_comparison['Class 0']

    # Sort by difference to find top distinguishing terms (both positive and negative)
    top_positive_terms = df_comparison.sort_values(by='difference', ascending=False).head(top_n)
    top_negative_terms = df_comparison.sort_values(by='difference', ascending=True).head(top_n)

    # Display top 10 terms with highest positive and negative difference
    print("Top positive terms:")
    print(top_positive_terms)
    print("\nTop negative terms:")
    print(top_negative_terms)
    # Concatenate top positive and negative terms
    top_terms_combined = pd.concat([top_positive_terms, top_negative_terms])

    # Extract the selected feature names
    selected_features = top_terms_combined.index.tolist()
    return selected_features

# Preprocess the mapping dictionary and compile regex patterns
map_dict = {
    'poc': ['proof of concept', 'poc', 'proof of principle', 'demo'],
    'mem_corruption': ['memory corruption', 'buffer overflow', 'stack smashing', 'buffer overrun', 'memory overflow',
                       'stack overrun', 'heap smashing'],
    'execution': ['code execution'],
    'dos': ['denial of service', 'ddos'],
    'remote': ['non local', 'distant'],
    'web': ['internet']
}

compiled_patterns = {}
for key, phrases in map_dict.items():
    for phrase in phrases:
        # Assume preprocess_text is defined elsewhere and used for preprocessing
        processed_phrase = preprocess_text(phrase)
        pattern = re.compile(r'\b' + re.escape(processed_phrase) + r'\b')
        compiled_patterns[pattern] = key

def map_vul_word(summary):
    # Splitting the summary into words for word-by-word comparison
    words = summary.split()
    mapped_words = []

    i = 0
    while i < len(words):
        found = False
        for pattern, key in compiled_patterns.items():
            match = pattern.match(' '.join(words[i:]))
            if match:
                mapped_words.append(key)
                i += len(match.group().split()) - 1  # Skip the length of the phrase less one
                found = True
                break
        if not found:
            mapped_words.append(words[i])
        i += 1

    # Rejoining the words into a mapped summary
    mapped_summary = ' '.join(mapped_words)
    return mapped_summary

def evaluate_xgboost_gridsearch(n_splits, X, y):
    # Parameters found in a previous gridsearch
    # Updating parameters for XGBoost
    # model_parameters = {'max_depth': 4, 'n_estimators': 100, 'learning_rate': 0.1,
    #                     'scale_pos_weight': sum(y == 0) / sum(y == 1)  # Adjust for imbalance
    #                     }

    # model_parameters = {'colsample_bytree': 0.7, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 6, 'min_child_weight': 1,
    #              'n_estimators': 200, 'reg_alpha': 0.01, 'reg_lambda': 0.1, 'subsample': 0.9, 'scale_pos_weight': sum(y == 1) / sum(y == 0)}

    # Parameter grid to search
    param_grid = {
        'max_depth': [6],
        'n_estimators': [200],
        'learning_rate': [0.1],
        'min_child_weight': [1, 2],
        'gamma': [0],
        'subsample': [0.9],
        'colsample_bytree': [0.7],
        'reg_alpha': [0.01],
        'reg_lambda': [0.1],
        # You can add or remove parameters as needed
    }
    # with grid search accuracy=roc
    # Best
    # Parameters: {'colsample_bytree': 0.7, 'gamma': 0.1, 'learning_rate': 0.1, 'max_depth': 6, 'min_child_weight': 1,
    #              'n_estimators': 200, 'reg_alpha': 0.01, 'reg_lambda': 0.1, 'subsample': 0.8}
    # Best
    # Score: 0.7424883985797457
    # param_grid = {
    #     'max_depth': [3, 4, 5, 6],
    #     'n_estimators': [50, 100, 150, 200],
    #     'learning_rate': [0.01, 0.05, 0.1, 0.2],
    #     'min_child_weight': [1, 2, 3, 4],
    #     'gamma': [0, 0.1, 0.2, 0.3],
    #     'subsample': [0.6, 0.7, 0.8, 0.9],
    #     'colsample_bytree': [0.6, 0.7, 0.8, 0.9],
    #     'reg_alpha': [0, 0.01, 0.1, 1],
    #     'reg_lambda': [0, 0.01, 0.1, 1],
    #     # You can add or remove parameters as needed
    # }

    # Initialize the classifier
    clf = XGBClassifier(scale_pos_weight=sum(y == 1) / sum(y == 0))  # Adjust for imbalance

    # Setup GridSearchCV
    kfold = StratifiedKFold(n_splits=n_splits, shuffle=True)
    grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='roc_auc', n_jobs=-1, cv=kfold, verbose=2)

    # Fit the grid search
    grid_search.fit(X, y)  # Note: You may want to use resampling techniques like SMOTE here

    # Best parameters and best score
    print("Best Parameters:", grid_search.best_params_)
    print("Best Score:", grid_search.best_score_)

    # You can use the best estimator directly from grid_search
    best_clf = grid_search.best_estimator_

    # Rest of your evaluation code...

    # Updating the classifier to XGBClassifier
    clf = best_clf  # XGBClassifier(**model_parameters)

    kfold = StratifiedKFold(n_splits=n_splits, shuffle=True)

    accuracy = []
    precision = []
    recall = []
    f1 = []
    roc_auc, f2 = [], []  # For storing F2 scores

    folds_thresholds = []
    folds_y, folds_accuracy = [], []
    print("Starting 10-fold cross validation")

    # We do 10-fold cross-validation to have the most accurate evaluation.
    # Accuracy, precision, recall and f1 score are used and the average is printed
    for train, test in kfold.split(X, y):
        clf.fit(X[train], y[train])
        probas = clf.predict_proba(X[test])
        pos_probas = probas[:, 1]

        # Find the best threshold for this fold
        # threshold,num_y,y_accuracy = find_threshold_with_all_exploited(y[test], pos_probas)#
        threshold = find_threshold(y[test], pos_probas)
        folds_thresholds.append(threshold)
        prediction = (probas[:, 1] >= threshold).astype(int)

        scores = clf.score(X[test], y[test])

        accuracy.append(accuracy_score(y[test], prediction) * 100)
        precision.append(precision_score(y[test], prediction) * 100)
        recall.append(recall_score(y[test], prediction) * 100)
        f1.append(fbeta_score(y[test], prediction, beta=1) * 100)
        f2.append(fbeta_score(y[test], prediction, beta=2) * 100)  # Calculating F2 score
        roc_auc.append(roc_auc_score(y[test], prediction) * 100)  # Calculating ROC AUC score

    print(f"Accuracy: {np.mean(accuracy):.2f}% (+/- {np.std(accuracy):.2f}%)")
    print(f"Precision: {np.mean(precision):.2f}% (+/- {np.std(precision):.2f}%)")
    print(f"Recall: {np.mean(recall):.2f}% (+/- {np.std(recall):.2f}%)")
    print(
        f"F1 score: {np.mean(f1):.2f}% (+/- {np.std(f1):.2f}%)")  # F2 score accepting more false positive and ignore false negetive
    print(f"F2 score: {np.mean(f2):.2f}% (+/- {np.std(f2):.2f}%)")  # Adding F2 score output
    print(f"ROC AUC score: {np.mean(roc_auc):.2f}% (+/- {np.std(roc_auc):.2f}%)")  # Outputting ROC AUC score
    print(f"Threshold: {np.mean(folds_thresholds):.2f}")
    # Return the average threshold to be used in the model fit
    return np.mean(folds_thresholds)

def evaluate_xgboost_t(n_splits, X, y):
    # Prepare for 10-fold cross-validation
    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

    # Define thresholds
    thresholds = np.arange(0.1, 1.0, 0.05)

    # Dictionary to store metrics for each threshold
    metrics = {thresh: {'accuracy': [], 'precision': [], 'recall': [], 'true_positives': [], 'true_negatives': [],
                        'false_positives': [],
                        'false_negatives': []} for thresh in thresholds}
    for train_index, test_index in kf.split(X, y):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        # Calculate scale_pos_weight
        class_count_0, class_count_1 = np.bincount(y_train)
        print(f'class_0:{class_count_0}|class_1:{class_count_1}')
        scale_pos_weight = class_count_0 / class_count_1
        test_count_0, test_count_1 = np.bincount(y_test)
        print(f'test_class_0:{test_count_0}|test_class_1:{test_count_1}')

        # Set up XGBoost parameters
        params = {
            'objective': 'reg:logistic',
            'max_depth': 4,
            'alpha': 10,
            'learning_rate': 0.01,
            'scale_pos_weight': scale_pos_weight,
            'n_estimators': 100
        }

        model = xgb.XGBClassifier(**params)

        sample_weight = np.where(y_train == 1, 5, 1)

        # Train the model with sample weights
        model.fit(X_train, y_train, sample_weight=sample_weight)
        probabilities = model.predict_proba(X_test)[:, 1]
        # Evaluate the model
        accuracy = model.score(X_test, y_test)
        print("Accuracy:", accuracy)
        # Calculate metrics for each threshold
        for thresh in thresholds:  # unique_thresholds:
            predictions = (probabilities >= thresh).astype(int)
            true_positives = np.sum((predictions == 1) & (y_test == 1))
            true_negetives = np.sum((predictions == 0) & (y_test == 0))
            false_positives = np.sum((predictions == 1) & (y_test == 0))
            false_negatives = np.sum((predictions == 0) & (y_test == 1))

            metrics[thresh]['accuracy'].append(accuracy_score(y_test, predictions))
            metrics[thresh]['precision'].append(precision_score(y_test, predictions, zero_division=0))
            metrics[thresh]['recall'].append(recall_score(y_test, predictions, zero_division=0))
            metrics[thresh]['true_positives'].append(true_positives)
            metrics[thresh]['true_negatives'].append(true_negetives)
            metrics[thresh]['false_positives'].append(false_positives)
            metrics[thresh]['false_negatives'].append(false_negatives)

        # Print average metrics for each threshold
        print('Threshold|Accuracy|Precision|recall|true_positive|true_negetive|false_positive|false_negetive ')

        for thresh in thresholds:  # unique_thresholds:
            avg_acc = np.mean(metrics[thresh]['accuracy'])
            avg_prec = np.mean(metrics[thresh]['precision'])
            avg_recall = np.mean(metrics[thresh]['recall'])
            avg_tp = np.mean(metrics[thresh]['true_positives'])
            avg_tn = np.mean(metrics[thresh]['true_negatives'])
            avg_fp = np.mean(metrics[thresh]['false_positives'])
            avg_fn = np.mean(metrics[thresh]['false_negatives'])
            print(
                f'{thresh:.4f} | {avg_acc:.4f} |  {avg_prec:.4f}|{avg_recall:.4f}|{avg_tp:.4f}|{avg_tn:.4f}|{avg_fp:.4f}|{avg_fn:.4f} ')

def get_synonym(word):
    synonyms = set()
    for syn in wordnet.synsets(word):
        for lemma in syn.lemmas():
            synonyms.add(lemma.name())
    if synonyms:
        return synonyms.pop()  # Return one synonym as representative
    return word

def evaluate_regression(n_splits, X, y, model_type='ridge'):
    # Ridge Regression parameters (alpha is the regularization strength)
    model_parameters = {'alpha': 1.0}  # Default value, adjust as needed

    # Choose between Ridge and Lasso
    if model_type == 'ridge':
        model = Ridge(**model_parameters)
    elif model_type == 'lasso':
        from sklearn.linear_model import Lasso
        model = Lasso(**model_parameters)
    else:
        raise ValueError("model_type must be 'ridge' or 'lasso'")

    kfold = KFold(n_splits=n_splits, shuffle=True)

    mse_scores = []
    r2_scores = []

    print("Starting k-fold cross validation for regression")

    for train, test in kfold.split(X, y):
        model.fit(X[train], y[train])
        predictions = model.predict(X[test])

        mse_scores.append(mean_squared_error(y[test], predictions))
        r2_scores.append(r2_score(y[test], predictions))

    print(f"Mean Squared Error (MSE): {np.mean(mse_scores):.2f} (+/- {np.std(mse_scores):.2f})")
    print(f"R-squared: {np.mean(r2_scores):.2f} (+/- {np.std(r2_scores):.2f})")

    return np.mean(mse_scores), np.mean(r2_scores)

def evaluate_imbalanced_classification_bernoulli(n_splits, X, y):
    # Instantiate the Bernoulli Naive Bayes classifier
    clf = BernoulliNB()

    kfold = StratifiedKFold(n_splits=n_splits, shuffle=True)

    accuracy = []
    precision = []
    recall = []
    f1 = []
    roc_auc = []
    pr_auc = []

    print("Starting k-fold cross validation for classification with Bernoulli Naive Bayes")

    for train, test in kfold.split(X, y):
        clf.fit(X[train], y[train])
        predictions = clf.predict(X[test])
        probas = clf.predict_proba(X[test])[:, 1]  # Probabilities for the positive class

        accuracy.append(accuracy_score(y[test], predictions))
        precision.append(precision_score(y[test], predictions, average='binary'))
        recall.append(recall_score(y[test], predictions, average='binary'))
        f1.append(f1_score(y[test], predictions, average='binary'))
        roc_auc.append(roc_auc_score(y[test], probas))
        pr_auc.append(average_precision_score(y[test], probas))

    print(f"Accuracy: {np.mean(accuracy):.2f}% (+/- {np.std(accuracy):.2f}%)")
    print(f"Precision: {np.mean(precision):.2f}% (+/- {np.std(precision):.2f}%)")
    print(f"Recall: {np.mean(recall):.2f}% (+/- {np.std(recall):.2f}%)")
    print(f"F1 score: {np.mean(f1):.2f}% (+/- {np.std(f1):.2f}%)")
    print(f"ROC AUC score: {np.mean(roc_auc):.2f} (+/- {np.std(roc_auc):.2f})")
    print(f"Precision-Recall AUC score: {np.mean(pr_auc):.2f} (+/- {np.std(pr_auc):.2f})")
    return np.mean(accuracy), np.mean(precision), np.mean(recall), np.mean(f1), np.mean(roc_auc), np.mean(pr_auc)

def evaluate_imbalanced_classification_bernoulli_with_oversampling(n_splits, X, y):
    # Instantiate the Bernoulli Naive Bayes classifier
    clf = BernoulliNB()

    kfold = StratifiedKFold(n_splits=n_splits, shuffle=True)

    accuracy = []
    precision = []
    recall = []
    f1 = []
    roc_auc = []
    pr_auc = []

    print("Starting k-fold cross validation with oversampling for Bernoulli Naive Bayes")

    for train, test in kfold.split(X, y):
        # Apply SMOTE to generate synthetic samples for the minority class
        smote = SMOTE()
        X_train_resampled, y_train_resampled = smote.fit_resample(X[train], y[train])

        clf.fit(X_train_resampled, y_train_resampled)
        predictions = clf.predict(X[test])
        probas = clf.predict_proba(X[test])[:, 1]  # Probabilities for the positive class

        accuracy.append(accuracy_score(y[test], predictions))
        precision.append(precision_score(y[test], predictions, average='binary'))
        recall.append(recall_score(y[test], predictions, average='binary'))
        f1.append(f1_score(y[test], predictions, average='binary'))
        roc_auc.append(roc_auc_score(y[test], probas))
        pr_auc.append(average_precision_score(y[test], probas))

    print(f"Accuracy: {np.mean(accuracy):.2f}% (+/- {np.std(accuracy):.2f}%)")
    print(f"Precision: {np.mean(precision):.2f}% (+/- {np.std(precision):.2f}%)")
    print(f"Recall: {np.mean(recall):.2f}% (+/- {np.std(recall):.2f}%)")
    print(f"F1 score: {np.mean(f1):.2f}% (+/- {np.std(f1):.2f}%)")
    print(f"ROC AUC score: {np.mean(roc_auc):.2f} (+/- {np.std(roc_auc):.2f})")
    print(f"Precision-Recall AUC score: {np.mean(pr_auc):.2f} (+/- {np.std(pr_auc):.2f})")

    return np.mean(accuracy), np.mean(precision), np.mean(recall), np.mean(f1), np.mean(roc_auc), np.mean(pr_auc)

def epss():
    df = pd.read_pickle('C:/Ava/Raphael/vulnerability-prediction/EPSS/2020_valid_cve_epss.pkl')

    unique_thresholds = np.linspace(np.min(df['EPSS']), np.max(df['EPSS']), 10)

    # Calculate metrics for each threshold
    for thresh in unique_thresholds:
        df['predictions'] = np.where(df['EPSS'] >= thresh, 1, 0)
        true_positives = len(df[(df['predictions'] == 1) & (df['exploited_in_wild'] == 1)])
        true_negetives = len(df[(df['predictions'] == 0) & (df['exploited_in_wild'] == 0)])
        false_positives = len(df[(df['predictions'] == 1) & (df['exploited_in_wild'] == 0)])
        false_negatives = len(df[(df['predictions'] == 0) & (df['exploited_in_wild'] == 1)])

        print(f'Threshold:{thresh}|TP:{true_positives}|TN:{true_negetives}|FP:{false_positives}|FN:{false_negatives}')

    # Sort the DataFrame based on 'EPSS' in descending order
    df_sorted = df.sort_values(by='EPSS', ascending=False)

    thresholds = np.arange(0, 1, 0.1)

    # Calculate metrics for the top 10% high EPSS values

    for thresh in thresholds:
        # Calculate the number of rows for the top 10%
        top_percent_rows = int(len(df_sorted) * thresh)

        # Filter the DataFrame to include only the top 10% of rows
        df_top_percent = df_sorted.head(top_percent_rows)

        true_positives = len(
            df_top_percent[(df_top_percent['exploited_in_wild'] == 1)])
        true_negatives = len(
            df_top_percent[(df_top_percent['exploited_in_wild'] == 0)])

        print(
            f'Threshold: {thresh:.2f} | TP: {true_positives} | TN: {true_negatives} ')

def exploitability_prediction():
    df = pd.read_pickle('2020_cve_all_features_1_3_2024.pkl')
    # preper other features
    features_df = df[
        ['cve_id', 'Source', 'vendor_employee_size_clusters', 'total_summary_word_count', 'vendors', 'products',
         'vendor_graynoise_reports_count_cluster', 'open_source_vendors_products']]
    features_encoded = pd.get_dummies(features_df, columns=['Source'])

    # Process each array column
    def process_array_column(df, column_name):
        # Explode the column
        exploded_df = df[['cve_id', column_name]].explode(column_name)

        # Perform one-hot encoding
        one_hot = pd.get_dummies(exploded_df[column_name])

        # Group by 'id' and sum
        grouped = one_hot.groupby(exploded_df['cve_id']).sum().reset_index()
        return grouped.drop('cve_id', axis=1)

    # Process each column
    one_hot_vendors = process_array_column(features_encoded, 'vendors')
    one_hot_products = process_array_column(features_encoded, 'products')
    one_hot_vendor_graynoise_clusters = process_array_column(features_encoded, 'vendor_graynoise_reports_count_cluster')
    one_hot_vebdor_employees = process_array_column(features_encoded, 'vendor_employee_size_clusters')

    # Merge the results with the original DataFrame
    # Add an index to merge properly
    one_hot_vendors.index = features_encoded.index
    one_hot_products.index = features_encoded.index
    one_hot_vendor_graynoise_clusters.index = features_encoded.index
    one_hot_vebdor_employees.index = features_encoded.index
    # Merge the results with the original DataFrame
    df_merged = pd.concat([features_encoded, one_hot_vendors, one_hot_products, one_hot_vendor_graynoise_clusters,
                           one_hot_vebdor_employees], axis=1)
    drop_columns = ['vendor_employee_size_clusters', 'vendors', 'products', 'vendor_graynoise_reports_count_cluster']
    df_merged = df_merged.drop(drop_columns, axis=1)

    df['summary_mapped'] = df['clean_summary'].apply(map_vul_word)
    tfidf_vectorizer = TfidfVectorizer(ngram_range=(2, 3))
    tfidf_matrix = tfidf_vectorizer.fit_transform(df['summary_mapped'])

    # Convert TF-IDF matrix to dense format if it's in sparse format
    tfidf_dense = tfidf_matrix.toarray()

    # Convert df_merged to a numpy array
    df_merged_array = df_merged.drop('cve_id', axis=1).values  # Dropping 'id' if it's not a feature

    # Concatenate the two arrays
    df_merged_encoded = pd.get_dummies(df_merged.drop('cve_id', axis=1))
    df_merged_encoded = df_merged_encoded.astype(int)
    df_merged_array = df_merged_encoded.values
    combined_features_sparse = hstack([tfidf_matrix, df_merged_array])

    # Feature Reduction
    from sklearn.decomposition import TruncatedSVD
    svd = TruncatedSVD(n_components=50)  # tryed 100 dimensions
    reduced_features = svd.fit_transform(combined_features_sparse)

    X = reduced_features

    df[LABEL] = df[['clam_report', 'cisa_report', 'secureworks_report', 'graynoise_report']].any(axis=1).astype(
        int)
    y = df[LABEL]  # Target
    evaluate_xgboost_t(10, X, y)
    evaluate_xgboost_gridsearch(10, X, y)
    del df_merged
    del df_merged_encoded
    del df_merged_array
    del one_hot_vendors
    del one_hot_products
    del one_hot_vebdor_employees
    del one_hot_vendor_graynoise_clusters
    del tfidf_dense
    del tfidf_matrix
    del tfidf_vectorizer
    # Model training and evaluation
    evaluate(10, X, y)
    evaluate_imbalanced_classification_bernoulli(10,X,y)
    evaluate_xgboost_t(10,X,y)
    evaluate_imbalanced_classification_bernoulli_with_oversampling(10,X,y)
    evaluate_regression(10,X,y)

if __name__ == '__main__':
    epss()
    exploitability_prediction()
